%% 07_bibliography:
%  Currently populated with example citations from MQP, replace with actual citations.

@BOOKLET{introToML,
	TITLE = {Introduction to Machine Learning},
	AUTHOR = {Nils J. Nilsson},
	YEAR = {2005}, 
	HOWPUBLISHED = {Web-published},
}

@BOOK{rlIntro,
	TITLE = {Reinforcement Learning: An Introduction},
	AUTHOR = {Richard S. Sutton and Andrew G. Barto},
	YEAR = {2017}, 
	PUBLISHER = {MIT Press},
	EDITION = {Second}
}

@incollection{backpropIntro,
	title = {How the backpropagation algorithm works},
	author = {Michael A Nielsen},
	crossref = {backpropBook},
	chapter = {2},
}

@book{backpropBook,
	TITLE = {Neural Networks and Deep Learning},
	BOOKTITLE={Neural Networks and Deep Learning},
	PUBLISHER = {Determination Press},
	AUTHOR = {Michael A Nielsen},
	YEAR = {2015},
	
}

@inbook{cartIntro,
	author      = {T. Hill and P. Lewicki},
	title       = "Popular Decision Tree: Classification and Regression Trees (C\&RT)",
	editor      = "",
	booktitle   = "Electronic Statistics Textbook",
	publisher   = "Statsoft, Inc.",
	address     = "",
	year        = 2013,
	pages       = "",
	chapter     = "9",
}

@inproceedings{rlmBasis,
	abstract = "This paper proposes a recursive Levenberg-Marquardt (LM) search direction as the training algorithm for non-linear adaptive filters, which use multi-layer feed forward neural nets as the filter structures. The neural nets can be considered as a class of non-linear adaptive filters with transversal or recursive filter structures. In the off-line training, the LM method is regarded as an intermediate method between the steepest descent (SD) and Gauss-Newton (GN) methods, and it has better convergence properties than the other two methods. In the echo cancellation experiments, the recursive LM algorithm converges faster and gives higher echo return loss enhancement (ERLE) than the recursive SD and GN algorithms.",
	author = "Ngia, L.S.H. and Sjoberg, J. and Viberg, M.",
	isbn = "0780351487",
	issn = "1058-6393",
	journal = "Signals, Systems & Computers, 1998. Conference Record of the Thirty-Second Asilomar Conference on",
	keywords = "Data- Och Informationsvetenskap ; Computer And Information Science ; Informationsteknik ; Information Technology;",
	language = "eng",
	pages = "697,701",
	publisher = "IEEE Publishing",
	title = "Adaptive neural nets filter using a recursive Levenberg-Marquardt search direction",
	volume = "1",
	year = "1998",
}

@inproceedings{rlmDetails,
	abstract = "A novel decomposed recursive Levenberg Marquardt (RLM) algorithm is derived for the training of feedforward neural networks. By neglecting interneuron weight correlations the recently proposed RLM training algorithm can be decomposed at neuron level enabling weights to be updated in an efficient parallel manner. A separable least squares implementation of decomposed RLM is also introduced. Experiment results for two nonlinear time series problems demonstrate the superiority of the new training algorithms.",
	author = "Asirvadam, V.S. and Mcloone, S.F. and Irwin, G.W.",
	address = "USA",
	isbn = "0-7803-7616-1",
	journal = "Neural Networks for Signal Processing, 2002. Proceedings of the 2002 12th IEEE Workshop on",
	keywords = "Computing and Processing ; Components, Circuits, Devices and Systems ; Signal Processing and Analysis",
	language = "eng",
	pages = "129,138",
	publisher = "IEEE",
	title = "Parallel and separable recursive Levenberg-Marquardt training algorithm",
	volume = "2002-",
	year = "2002",
}

%% PAPERS THAT are direct precedents.
@INPROCEEDINGS{paulo_theory_paper, 
author={P. V. R. Ferreira and R. Paffenroth and A. M. Wyglinski and T. M. Hackett and S. G. Bilén and R. C. Reinhart and D. J. Mortensen}, 
booktitle={2017 Cognitive Communications for Aerospace Applications Workshop (CCAA)}, 
title={Multi-objective reinforcement learning-based deep neural networks for cognitive space communications}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-8}, 
keywords={cognitive radio;learning (artificial intelligence);neural nets;resource allocation;satellite communication;software radio;space communication links;telecommunication computing;DVB-S2 standard adaptive transmitter parameters;International Space Station;NASA Glenn Research Center SCaN Testbed;SDR;adaptive radio systems;core cognitive engine proof-of-concept;deep artificial neural networks;future critical space-based missions;hybrid radio resource allocation management control algorithm;machine learning algorithms;multiobjective reinforcement learning;online learning;performance functions;radio parameters;satellite communication channel;software-defined radios;space exploration missions;transmitter parameter adaptation;virtual environment exploration;Artificial neural networks;Learning (artificial intelligence);NASA;Prediction algorithms;Resource management;Space communications;NASA GRC;SCaN Testbed;Satellite communication;artificial intelligence;cognitive radio;machine learning;neural networks;reinforcement learning;space communication}, 
doi={10.1109/CCAAW.2017.8001880}, 
ISSN={}, 
month={June},}


@INPROCEEDINGS{tim_implementation, 
author={T. M. Hackett and S. G. Bilén and P. V. R. Ferreira and A. M. Wyglinski and R. C. Reinhart}, 
booktitle={2017 Cognitive Communications for Aerospace Applications Workshop (CCAA)}, 
title={Implementation of a space communications cognitive engine}, 
year={2017}, 
volume={}, 
number={}, 
pages={1-7}, 
keywords={learning (artificial intelligence);neural nets;software architecture;software libraries;space communication links;DVB-S2 standard;NASA Glenn research center;deep artificial neural networks;modular software architecture;multiobjective reinforcement-learning algorithm;radio-resource-allocation-management controller;real-world radio constraints;software libraries;space communications cognitive engine;Artificial neural networks;Engines;Licenses;NASA;Software libraries;Space vehicles;SCaN Testbed;cognitive engine;machine learning;neural networks;reinforcement learning;space communications}, 
doi={10.1109/CCAAW.2017.8001607}, 
ISSN={}, 
month={June},}

%% Reference papers, Iterative learning:

% PUT THESE IN LATER:
@ARTICLE{NSE_learning_review, 
author={G. Ditzler and M. Roveri and C. Alippi and R. Polikar}, 
journal={IEEE Computational Intelligence Magazine}, 
title={Learning in Nonstationary Environments: A Survey}, 
year={2015}, 
volume={10}, 
number={4}, 
pages={12-25}, 
keywords={Internet of Things;computer aided instruction;mobile handsets;mobile learning;probability;statistical distributions;Internet-of-things technology;aging effects;cyber-physical system;hardware faults;mobile phones;nonadaptive model;nonstationary environments;periodicity effects;probability distribution;sensor networks;software faults;thermal drifts;user habits;Adaptation models;Algorithm design and analysis;Behavioral science;Biological system modeling;Feature extraction;Learning systems;Probability distribution;Sensor phenomena and characterization;Training}, 
doi={10.1109/MCI.2015.2471196}, 
ISSN={1556-603X}, 
month={Nov},}

@INPROCEEDINGS{inc_learning_review, 
author={A. Gepperth and B. Hammer}, 
booktitle={2016 European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning}, 
title={Incremental learning algorithms and applications}, 
year={2016}, 
volume={}, 
number={}, 
pages={357-368}, 
month={April},}

@article{empericalCatForget,
	abstract = {Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models "forget" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.},
	author = "Goodfellow, Ian J. and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua",
	keywords = "Statistics - Machine Learning ; Computer Science - Learning ; Computer Science - Neural And Evolutionary Computing",
	title = "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks",
	year = "2013-12-21",
}

@inproceedings{mlEnsemble,
	address = "Berlin ;",
	booktitle = "Multiple classifier systems : first international workshop, MCS 2000, Cagliari, Italy, June 21-23, 2000 : proceedings",
	isbn = "3540677046",
	keywords = "Machine learning",
	language = "eng",
	lccn = "00044679",
	publisher = "Springer",
	series = "Lecture notes in computer science, 1857",
	title = "Multiple classifier systems : first international workshop, MCS 2000, Cagliari, Italy, June 21-23, 2000 : proceedings ",
	year = "2000",
}


@article{ganReview,
	abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
	author = "Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.",
	address = "USA",
	issn = "1053-5888",
	journal = "Signal Processing Magazine, IEEE",
	keywords = "Machine Learning ; Generators ; Training Data ; Data Models ; Convolutional Codes ; Image Resolution ; Signal Resolution ; Semantics ; Signal Processing and Analysis",
	language = "eng",
	number = "1",
	pages = "53,65",
	publisher = "IEEE",
	title = "Generative Adversarial Networks: An Overview",
	volume = "35",
	year = "2018-01",
}

---------------------
https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220 GRADIENT DESCENT \\
http://people.duke.edu/~hpgavin/ce281/lm.pdf LEVENBERG MARQUARDT \\
Paper on adaboost R2
Paper on pruning trees, have print copy.

% ENSEMBLE THINGS

@inproceedings{learnNseIntro,
	abstract = "We have recently introduced an incremental learning algorithm, called Learn++.NSE, designed for Non-Stationary Environments (concept drift), where the underlying data distribution changes over time. With each dataset drawn from a new environment, Learn++.NSE generates a new classifier to form an ensemble of classifiers. The ensemble members are combined through a dynamically weighted majority voting, where voting weights are determined based on classifiers' age-adjusted accuracy on current and past environments. Unlike other ensemble-based concept drift algorithms, Learn++.NSE does not discard prior classifiers, allowing potentially cyclical environments to be learned more effectively. While Learn++.NSE has been shown to work well on a variety of concept drift problems, a potential shortcoming of this approach is the cumulative nature of the ensemble size. In this contribution, we expand our analysis of the algorithm to include various ensemble pruning methods to introduce controlled forgetting. Error or age-based pruning methods have been integrated into the algorithm to prevent potential out-voting from irrelevant classifiers or simply to save memory over an extended period of time. Here, we analyze the tradeoff between these precautions and the desire to handle recurring contexts (cyclical data). Comparisons are made using several scenarios that introduce various types of drift.",
	author = "Elwell, Ryan and Polikar, Robi",
	isbn = "9781424435487",
	issn = "1098-7576",
	journal = "Neural Networks, 2009. IJCNN 2009. International Joint Conference on",
	language = "eng",
	pages = "771,778",
	publisher = "IEEE Publishing",
	title = "Incremental learning in nonstationary environments with controlled forgetting",
	year = "2009-06",
}

@INPROCEEDINGS{1007781, 
author={R. Polikar and J. Byorick and S. Krause and A. Marino and M. Moreton}, 
booktitle={Neural Networks, 2002. IJCNN '02. Proceedings of the 2002 International Joint Conference on}, 
title={Learn++: a classifier independent incremental learning algorithm for supervised neural networks}, 
year={2002}, 
volume={2}, 
number={}, 
pages={1742-1747}, 
keywords={learning (artificial intelligence);neural nets;Learn++;classifier independent incremental learning algorithm;supervised neural network classifier;supervised neural networks;synergistic expressive power;weak classifiers;Availability;Computer networks;Function approximation;Inference algorithms;Machine learning;Neural networks;Pattern recognition;Power engineering and energy;Power engineering computing;Stability}, 
doi={10.1109/IJCNN.2002.1007781}, 
ISSN={1098-7576}, 
month={},}
% The background in the following paper is useful, but the actual concept isn't what we're trying to do at all.
@INPROCEEDINGS{6033578, 
author={G. Ditzler and R. Polikar}, 
booktitle={The 2011 International Joint Conference on Neural Networks}, 
title={Semi-supervised learning in nonstationary environments}, 
year={2011}, 
volume={}, 
number={}, 
pages={2741-2748}, 
keywords={Gaussian processes;data analysis;learning (artificial intelligence);pattern classification;Gaussian mixture model;data classifier;drifting distribution;learning concept drift;machine learning;nonstationary environment;semisupervised learning;streaming data;unlabelled data;voting weight;Algorithm design and analysis;Classification algorithms;Clustering algorithms;Data models;Testing;Training;Training data;concept drift;ensemble systems;incremental learning;non-stationary environments;unlabeled data}, 
doi={10.1109/IJCNN.2011.6033578}, 
ISSN={2161-4393}, 
month={July},}

%% REFERENCE PAPERS, GANS

@article{gan_overview,
  author    = {Antonia Creswell and
               Tom White and
               Vincent Dumoulin and
               Kai Arulkumaran and
               Biswa Sengupta and
               Anil A. Bharath},
  title     = {Generative Adversarial Networks: An Overview},
  journal   = {CoRR},
  volume    = {abs/1710.07035},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.07035},
  archivePrefix = {arXiv},
  eprint    = {1710.07035},
  timestamp = {Wed, 01 Nov 2017 19:05:43 +0100},
  biburl    = {http://dblp.org/rec/bib/journals/corr/abs-1710-07035},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

%% MAYBE USEFUL BUT ALSO MAYBE NOT

@ARTICLE{ALI_encdec_inference,
   author = {{Dumoulin}, V. and {Belghazi}, I. and {Poole}, B. and {Mastropietro}, O. and 
    {Lamb}, A. and {Arjovsky}, M. and {Courville}, A.},
    title = "{Adversarially Learned Inference}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1606.00704},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning},
     year = 2016,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160600704D},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



