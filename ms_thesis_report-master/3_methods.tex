\chapter{Methods}
\par In the following chapter, both the software and hardware details of the test platform will be described. First, the baseline software implementation of the Cognitive Engine will be described. After this, the modified training methods will be explained. These methods include Recursive Levenberg-Marquardt (RLM), as well as Learn++.NSE (NSE). The CE was first implemented in MATLAB, to verify functionality. This was then ported to C++ using the MLPack library. Both these implementations are described below. After discussing the software, the hardware setups for ground testing, validation, and flight testing will be described.
\section{Software Methods}

\subsection{CE Details}
\par The architecture of the CE is independent of the implementation method, and will be described in the following section. The overarching architecture follows that of a SARSA Reinforcement Learning algorithm. 
\par The engine begins by randomly selecting actions and observing the results of these actions, storing the action-reward pairs in a history buffer. Once this buffer, explore and exploit networks are trained. Once they complete training, the cognitive engine has completed its startup process.
\par During each following step, the $\epsilon$ value used to decide between exploration and exploitation is first recalculated. This is done by taking the inverse of an incrementing counter. When this value passes a threshold, the counter is reset and an exploration is forced. After this value is updated, a number is drawn from a uniformly random distribution $\mathcal{U}(0,1)$. Then, if this number is less than $\epsilon$, an explore iteration is triggered. Otherwise, an exploit iteration is triggered.
\par In the case of an exploration, all possible actions, as well as the observed SNR (normalized to [0,1] by dividing by the maximum SNR), are used as inputs to the Explore MLP (or MLPs in the case of an ensemble). The MLP returns a predicted fitness score for each action. From this, the actions are split up into groups of actions that have a fitness score greater than a threshold and actions that have a fitness score less than a threshold. Then, with a 95\% probability, an action is randomly selected from the group of actions with a greater fitness score than the threshold. The 5\% probability of selecting from the other group of actions allows for exploration of areas that the CE may have misinterpreted.
\par In the case of exploitation, The normalized observed reward values of the last action are used as inputs to the exploit MLPs. One MLP corresponds to each tuneable parameter, of which there are six. These six output values are then denormalized to get the actual output actions.
\par Regardless of how the action was chosen, it is then transmitted to the SDR platform (either in simulation or in reality). The values received afterwards are $Es/No$, powe rconsumed, power efficiency, bandwidth used, and throughput. These are all measured at the transmit side. At the receive side, a BER, spectral efficiency, and throughput are measured or calculated. Once these are all present, a fitness score is calculated, as a weighted combination of these values. \textbf{\textit{Don't forget to have table of weight combinations, and talk about how its normalized.}} If this value is greater than the maximum observed value previously, it becomes the next new maximum observed value, and if this value was achieved during exploration, the performance values are the next input to the exploit network. If the value is less than the maximum value and was chosen by exploitation, a new set of logic is followed. A value $e_p$ is kept, representing the maximum fitness value while accounting for slow drift in fitness value responses resulting from changing environment conditions. If the action taken was the same as the action before it, and the observed fitness is more than 0.1 less than the $e_p$, then it is assumed the environment has undergone a rapid shift. The history buffer is reset and the system is reset to the initial state, in which it randomly explores until the history buffer is filled.  If the observed fitness is more than 0.1 less than $e_p$ but a different action was chosen, then the CE finds the best performing action from the history buffer and uses that action. Otherwise, if the observed fitness is less than 0.1 less than $e_p$ and isn't the first sample in the buffer, this new action is accepted. If the observed fitness is less than 0.1 less than $e_p$ but no quick recover has occurred, the last known action is chosen as the new action to take. Finally, if the observed fitness is greater than $e_p$, $e_p$ gets updated, and the last exploitation action gets saved. 
\par Once this action-choosing logic has occurred, the observed fitness values are added to the history buffer. If the action chosen has not been chosen before, the results are directly added to the history buffer. Otherwise, the history updates the observed fitness of the action. If the history buffer is full, the explore and exploit networks get trained. Otherwise, the next iteration occurs.
\par Training occurs in two phases, one for explore and one for exploit. During each phase, data from the history buffer is randomly split up into training and testing sets. Then, the training algorithm is applied, whether it is LM, RLM, or NSE. Details of the implementation of each will be described below, in the language-specfic sections. The training methods were described as 
\par The Explore and Exploit MLPs had different architectures, but maintained some similarities. Both used logarithmic activation functions, as well as a linear output transfer function. The Explore MLP was composed of 3 layers. The first hidden layer has 7 nodes, the second hidden layer had 50 nodes, and produced 1 output. The Exploit MLP was actually 6 different MLPs, one for each parameter to be predicted. Each sub-MLP was 2 layers, with 7 nodes on the first hidden layer, 20 nodes on the second hidden layer and 1 output. In the past, each weight was initialized using MATLAB's default initialization method, Nguen-Widrow initialization. However, due to convergence issues with the RLM method, the weight initialization was changed to the Glorot (Xavier) initialization technique, which sets weights in a weighted gaussian random manner, as described below.
\begin{align*}
	var(W) &= \frac{2}{n_{in}+n_{out}}
\end{align*} 
\par $n_in$ and $n_out$ are the number of input and output nodes in the network.
\subsection{Simulation}
\textbf{\textit{CURRENTLY JUST ALL THE STUFF THAT WAS DONE, will need to actually stitch together..}} \\
\par Prior to this thesis, the CE was prototyped in MATLAB by Paulo Ferriera. It was developed in MATLAB 2015a, using the Parallel Computing Toolbox and the Neural Network Toolbox. It is important to notice that the Statistics and Machine Learning toolbox was used and not the Deep Learning toolbox. Both toolboxes are able to create the CE, but have different interfaces. The following section will describe in summary the structure of the baseline simulation, as well as the modifications required to implement RLM and NSE. The full MATLAB code base can be found in appendix (\ref{PLACEHOLDERREF_MATLABCODE}).
\par For all cases, the MATLAB neural network structure was kept as the base. However, for RLM and NSE, the framework was augmented with training functions more specific to the algorithms. Most of the previous implementation of the CE was untouched during the thesis. Indeed, the only part that had a major change is the training function, which is abstracted away be default. 
\par By the nature of RLM, there are a handful of parameters and intermediate values that need to be kept track of. This was done using a matlab struct, RecurseMatrix, found in appendix \ref{app:recurseMatrix}. It contains the current Gradient, time, $P$ matrix, $\rho$, $S$, $\alpha$, and the size of a batch to be trained.  
\par  The implementation of Learn++.NSE was modified from the version provided in the following github repository: https://github.com/gditzler/IncrementalLearning/tree/master/src \textbf{\textit{DON'T FORGET TO FIX THIS INTO AN ACTUAL CITATION LOL}}. \textbf{\textit{actually finish lol}}

\subsection{C++}
\par The software architecture used in ground and flight testing was initially introduced by the authors in [\cite{lol cite it}]. In order to ensure generality, performance, and reusability, an Object-Oriented programming langugae was most suitable. C++ was chosen, both for performance and for some of the constructs in the C++11 standard. In addition, many open-source C++ libraries exist that were helpful. The software was designed and compiled in an Ubuntu 16.04 Linux VM, but was designed to be recompiled for any x86-based operating system.
\par In the design of the CE, drivers for the multiple modems needed to be created. These drivers are restricted by the International Traffic in Arms Regulations (ITAR), meaning that the code needed to have proprietary components. This ruled out many "copyleft" licenses, such as the GNU General Public License (GPL). This severely reduces the pool of possible libraries to use. 
\par Despite the wide variety of publicly available machine learning libraries, many do not have APIs for lower-level languages such as C/C++. While this is rapidly changing with the development of more embedded machine learning applications, at the time of the initial CE construction, this was the case [\cite{lol tim paper}. Furthermore, many of the C/C++ libraries lacked in-depth documentation for the lower-level APIs, had very complex build processes, or were built for larger, more complex applications (such as deep, convolutional NNs). 
\par MLPack \cite{cite_MLPAck_lol} was the library chosen to implement the MLPs in the CE. Because of the time of development the version used was \textbf{\textit{FIND OUT VERSION NUMBER LOL}}. Since then, MLPack has revamped its ANN architecture, so the library version is important. MLPack is built on top of the Armadillo \cite{cite_armadillo_lol} library, so Armadillo was used for self-written matrix/vector operations and storage as well. By doing this, the interfacing between self-written components and MLPack was simple. 
\par In order to simplify the interfacing that the CE has to do with Ethernet and UDP/IP to communicate with modems (described in Section \ref{BG:hardware}), the Boost.ASIO [\cite{cite_boostasio_lol}] library was used. This library abstracts away any operating system-specific constructs for handling sockets.
\par Finally, the Boost.Serialization [\cite{cite_boostSerial_lol}] library was chosen for saving/resuming CE states. 
\par\textbf{\textit{Discuss baseline implementation}} 
\par A block diagram of the baseline implementation is shown in figure \ref{figure2FromTimsPaper}. The external drivers interact with the Viasat DVB-S2 modem (for getting $E_s/N_o$ values), the ML-605 BPSK transmitter (for sending new actions to the space DVB-S2 transmitter), and the Advanced Radio for Cognitive Communications (ARCC) for initial ground testing. The cognitive algorithm under test resides in the RLNN Core section. The description of the RLNN Core will be split into three parts: the baseline method used in 2017, the RLM changes, and the Learn++.NSE changes.
\par The explore and exploit MLP ensembles that were used were abstracted into an NN Predictor object, allowing for the MLPack libraries to be abstracted away. Within the NN Predictor object is the FeedForwardNetwork object, which implements the individual FFNN and the self-written implementation of Levenberg-Marquardt. As described in \ref{bg:onlineLearning}, the CE uses online training. This required an NN Predictor model to get trained up and NN Predictor model for execution of the model to allow for concurrent training and usage. Training occurs using one NN Predictor model, and then the weights get copied to the other NN Predictor model. 
\par The buffer of training data, which is common between Explore and Exploit NN Predictors, is implemented in a separate object. It holds the latest N unique actions. When training occurs, the buffer is split into training and validation sets for each type of MLP (as Explore and Exploit MLPs have different inputs and outputs). This uses the Armadillo library.
\par The Application Specific Module provides context for the application, in that the rest of the RLNN Core is generic and can be applied to problems outside of communications. The Appication Specific module is the component that transforms communication metrics into the fitness scores that the RLNN Core uses. Any patches that change system behavior occur in this module as well. One example of this is the patch that limits transmit power changes to 1.5 dB steps. Another example is that the fitness scores are zeroed out when BER was measured to be 0.5, as this implies that the system had no communication between transmitter and receiver. 
\par\textbf{\textit{Discuss RLM Changes}}
\par The key changes between the RLM implemenation and the normal LM implementation occurred in the FeedForwardNetwork object. Here, the LM function was replaced by a function that runs RLM. Because of the additional parameters that RLM uses, a separate class called RecursiveLMHelper was used to contain the parameters and abstract the matrix math in the RLM updating process.
\par\textbf{\textit{Discuss NSE Changes}}
\par Unlike RLM, Learn++.NSE was mostly implemented in a modified version of NeuralNetwork Predictor, renamed LearnNSEPredictor. This is because the foundational MLP aspects of training are the same as the LM implementation. Where a difference occurs is the weighting of the samples that are being trained on and how the predictor is used. As such, the main changes occur in the train and predict functions. The algorithm is described in \ref{bg:NSESection}, and so will not be reiterated here.
\section{hardware methods}\label{BG:hardware}
\par In the following section, the hardware used in the ground testing and the on-orbit testing.
\subsection{ground test setup}
\par During the ground tests that were conducted in July of 2018, the test setup was very similar to that used in 2017 [\cite{tim_implementation}].  Similarly to that work, a testbed at NASA GRC was used to create an emulation of the expected on-orbit environment. A simplified block diagram of the setup is shown in Fig. \ref{methods:groundTestFig}. The CE is contained in an Ubuntu 16.0.4 virtual machine on a Windows 7-based Dell T3600 Precision workstation. The VM was allocated 4 GB of dedicated memory, and shared all eight of the CPU cores (with hyper threading) with the host operating system. The CE is put on a private local area network with the ML-605 BPSK transmitter and the ViaSat DVB-S2 receiver. The ML-605 radio transmitted new actions to the S-band Radio engineering model \textbf{\textit{MAKE SURE THIS IS CORRECT TO WHAT WE ACTUALLY DID}}. The S-band Radio then transmitted DVB-S2 frames through a channel emulated by a variable attenuator, according to a measured SNR profile from a previous on-orbit experiment. The noise floor can be adjusted by the noise generator. All RF signals in this test were passed using coaxial cables, insetad of any antennas.
\begin{figure}[ht]
\label{methods:groundTestFig}
\caption{placeholder.}
\end{figure} 

\subsection{flight setup}
\par The setup for on-orbit testing was identical to the setup used in 2017 [\cite{tim_implementation}], which in turn used a setup initially used by previous NASA GRC collaborators. A simplified block diagram of the setup is shown in Fig. \ref{methods:flightTestFig}. In the chain, there are two DVB-S2 rx modems. The ViaSat modem was used for sending $E_s/N_0$ measurements at a rate of 100 Hz over UDP. The Newtec modem demodulates and decodes the actual frames coming in, and saves it to a binary file for postprocessing. During each dataframe, the CE saves the previous action tuple along with its performance. It then chooses the next action tuple, and sends the next action to the ML-605 BPSK modem, which then is used to uplink the signal to the SCaN Testbed.

\begin{figure}[ht]
\label{methods:flightTestFig}
\caption{placeholder.}
\end{figure} 

\section{Post-processing techniques???}
