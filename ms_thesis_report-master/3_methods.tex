\chapter{Methods}\label{ch:methods}
\par In the following chapter, both the software and hardware details of the test platform will be described. First, Language independent aspects of the baseline software implementation of the Cognitive Engine will be described. Then, the language dependent aspects of the implementation will be explained. After this, the modified training methods will be explained. These methods include Recursive Levenberg-Marquardt (RLM), as well as Learn++.NSE (NSE). The CE was first implemented in MATLAB, to verify functionality. This was then ported to C++ using the MLPack library. Both these implementations are described below. After discussing the software, the hardware setups for ground testing, validation, and flight testing will be described.
\section{Software Methods}

\subsection{CE Details}
\begin{figure}
\caption{placeholder figure for CE block diagram}
\end{figure}
\par The architecture of the CE is independent of the implementation method, and will be described in the following section. The overarching architecture follows that of a SARSA Reinforcement Learning algorithm, with certain aspects replaced by MLPs. 
\par The engine begins by randomly selecting actions and observing the results of these actions, storing action-reward pairs in a history buffer. In this experiment, the action is the specific configuration of PHY layer parameters, and the reward is a fitness score that is a weighted combination of different evaluation metrics, including thoughput, spectral efficiency, bandwidth used, power consumed on transmit, bit error rate (BER), and DC power consumed. Throughput, spectral efficiency, and bandwidth are all calculated based on the chosen set of PHY parameters, while DC power consumed and transmit power consumed are measured. BER comes from a table that was precalculated. The calculated parameters are described in more detail in Appendix \ref{app:PhyParams}. Each parameter gets scaled from 0 to 1 based on the values that can be expected from each input. Then, they get combined in a weighted manner to produce the resulting fitness score. The importance of each metric is hard to determine, and can change depending on what the system currently needs to do. Six different weight combinations were developed, and are shown in table \ref{table:fitMissions}.
\begin{table}
\centering
\begin{tabu} to 1.15\textwidth{|X[c]|X[c] X[c] X[c] X[c] X[c] X[c]|}
	\hline 
	Mission Name & Throughput&BER&Target BW&Spectral Efficiency&TX Efficiency&DC Power Consumed\\
	\hline
	Emergency& 0.1 &0.8&0.025&0.025&0.025&0.025 \\
	Cooperation&0.05&0.05&0.4&0.4&0.05&0.05\\
	Power Saving&0.05&0.05&0.05&0.05&0.3&0.5\\
	Balanced&1/6&1/6&1/6&1/6&1/6&1/6\\
	Launch&0.2&0.4&0.1&0.1&0.1&0.1\\
	Multimedia&0.5&0.3&0.05&0.05&0.05&0.05\\
	\hline
\end{tabu}
\caption{Table containing different ways fitness score can be weighted.}
\label{table:fitMissions}
\end{table}
\par Once this buffer has filled up, explore and exploit networks are trained. After completing training, the CE has completed its startup process. 
\par For the first step after training, the exploration threshold $\epsilon$ is set to 1, forcing an explore During each following step, the $\epsilon$ value is recalculated. This is done by taking the inverse of an incrementing counter. When this value passes a threshold, the counter is reset and an exploration is again forced. After $\epsilon$ is updated, a number is drawn from a uniformly random distribution $\mathcal{U}(0,1)$. Then, if this number is less than $\epsilon$, an explore iteration is triggered. Otherwise, an exploit iteration is triggered.
\par In the case of an exploration, all actions in the action space, as well as the observed SNR (normalized to [0,1] by dividing by the maximum SNR), are used as inputs to the \textit{\textbf{Explore MLP (or MLPs in the case of an ensemble).}} The MLP returns a predicted fitness score for each action. 

\par  From this, the actions are split up into two groups: actions that have a fitness score greater than the threshold and actions that have a fitness score less than the threshold. Then, with a 95\% probability, an action is randomly selected from the group of actions with a greater fitness score than the threshold. The 5\% probability of selecting from the other group of actions allows for exploration of areas that the Explore MLP may have misinterpreted or areas that have been insufficiently explored.
\par In the case of exploitation, The normalized observed reward values of the last action are used as inputs to the exploit MLPs. One MLP corresponds to each tuneable parameter, of which there are six. These six output values are then denormalized to get the actual output actions.
\par Regardless of how the action was chosen, it is then transmitted to the SDR platform (either in simulation or reality). The values received after transmission are $Es/No$, power consumed, power efficiency, bandwidth used, and throughput. These are all measured at the transmit side. At the receive side, a BER, spectral efficiency, and throughput are measured or calculated. Once these are all present, a fitness score is calculated, as a weighted combination of these values. Currently, these weights are somewhat arbitrarily picked to simulate theoretical potential use cases. However, these weights can be essentially whatever seems correct, and could potentially be chosen by AI as well. 
\begin{table}
\caption{placeholder for table containing fitness weights}
\end{table}
\par If the objective value is greater than the maximum previously observed value, it becomes the next maximum observed value, and if this value was achieved during exploration, the performance values are the next input to the exploit network. If the value is less than the maximum value and was chosen by exploitation, a new set of logic is followed. A value $e_p$ is kept, representing the maximum fitness value while accounting for slow drift in fitness value responses resulting from slow-changing environmental properties. sIf the action taken was the same as the action before it, and the observed fitness is more than 0.1 less than the $e_p$, then it is assumed the environment has undergone a rapid shift. The history buffer is reset and the system is reset to the initial state, in which it randomly explores until the history buffer is filled.  If the observed fitness is more than 0.1 less than $e_p$ but a different action was chosen, then the CE finds the best performing action from the history buffer and uses that action. Otherwise, if the observed fitness is less than 0.1 less than $e_p$ and isn't the first sample in the buffer, this new action is accepted. If the observed fitness is less than 0.1 less than $e_p$ but no quick recover has occurred, the last known action is chosen as the new action to take. Finally, if the observed fitness is greater than $e_p$, $e_p$ gets updated, and the last exploitation action gets saved. 


\begin{figure}
\caption{placeholder for flow diagram of history / action choice logic}
\end{figure}
\par Once this action-choosing logic has occurred, the observed fitness values are added to the history buffer. If the action chosen is unique to any action in the buffer, the results are directly added to the history buffer. Otherwise, the history updates the observed fitness of the action in the buffer. If the history buffer is full, the explore and exploit networks get trained. Otherwise, the next iteration of the CE occurs.
\par Training occurs in two phases, one for explore and one for exploit. During each phase, data from the history buffer is randomly split up into training and testing sets. Then, the training algorithm is applied, whether it is LM, RLM, or NSE. Details of the implementation of each will be described below, in the language-specfic sections. 
\par The Explore and Exploit MLPs had different architectures, but maintained some similarities. Both used logarithmic activation functions, as well as a linear output function. The Explore MLP was composed of 3 layers. The input layer takes 7 inputs, The first hidden layer has 7 nodes, the second hidden layer had 50 nodes, and the output layer has 1 node. The Exploit MLP was actually 6 different MLPs, one for each parameter to be predicted. Each sub-MLP was 2 layers, with 7  inputs, 20 nodes on the first hidden layer and 1 output. In the past, each weight was initialized using MATLAB's default initialization method, Nguen-Widrow initialization. However, due to convergence issues the RLM method encountered while using this, the weight initialization was changed to the Glorot (Xavier) initialization technique\cite{placeholder_glorotInit}, which sets weights in a weighted gaussian random manner, as described below.
\begin{align*}
	var(W) &= \frac{2}{n_{in}+n_{out}}
\end{align*} 
\par $n_{in}$ and $n_{out}$ are the number of input and output nodes in the network.
\subsection{Simulation}
\par Prior to this thesis, the CE was prototyped in MATLAB by Paulo Ferriera\cite{placeholder_pauloPaper}. It was developed in MATLAB 2015a, using the Parallel Computing Toolbox and the Neural Network Toolbox. It is important to notice that the Statistics and Machine Learning toolbox was used and not the Deep Learning toolbox. Both toolboxes are able to create the CE, but have different interfaces. The following section will describe in summary the structure of the baseline simulation, as well as the modifications required to implement RLM and NSE. The full MATLAB code base can be found in appendix (\ref{PLACEHOLDERREF_MATLABCODE}).
\par For all learning methods, the MATLAB neural network structure was jused as the foundation. However, for RLM and NSE, the framework was augmented with training functions more specific to each algorithm. Most of the previous implementation of the CE was untouched during the thesis. Indeed, the only part that had a major change is the training function, which is abstracted away by default. 
\par By the nature of RLM, there are a handful of parameters and intermediate values that need to be kept track of. This was done using a matlab struct, RecurseMatrix, found in appendix \ref{app:recurseMatrix}. It contains the current Gradient, time, $P$ matrix, $\rho$, $S$, $\alpha$, and the size of a batch to be trained. Beyond that, the algorithm described in \ref{bg:RLM} is followed in a straightforward way.   
\par  The implementation of Learn++.NSE was modified from the version provided in the following github repository: https://github.com/gditzler/IncrementalLearning/tree/master/src \textbf{\textit{DON'T FORGET TO FIX THIS INTO AN ACTUAL CITATION LOL}}. This uses a GPL license, but as the MATLAB code has no ITAR-restricted drivers, this doesn't pose an issue. The main changes to this code involved adjusting the structure to fit a regression problem instead of a classification problem. Adaboost.R1 was used as a reference point in doing this adjustment, as it is similar in operation, if not how it weighs samples \cite{lol_adaboost}. The cross-entropy loss function that was used got replaced by mean squared error. In addition, during prediction the result was a weighted average of the ensembles, instead of a weighted majority vote. Other than the changes necessary for turning a classifier to a regressor, not much was changed, beyond a handful of extra hyperparameters. 

\subsection{C++}
\par The software architecture used in ground and flight testing was initially introduced by the authors in \cite{placeholderCitation}. In order to ensure generality, performance, and reusability, an Object-Oriented programming langugae was most suitable. C++ was chosen, both for performance and for some of the constructs in the C++11 standard. In addition, many open-source C++ libraries exist that were helpful. The software was designed and compiled in an Ubuntu 16.04 Linux VM, but was designed to be recompilable for any x86-based operating system, and can, with work, be used with most processor types that have a C++ compiler.
\par In the design of the CE, drivers for the multiple modems needed to be created. These drivers are restricted by the International Traffic in Arms Regulations (ITAR), meaning that the code needed to have proprietary components. This ruled out many "copyleft" licenses, such as the GNU General Public License (GPL). This severely reduces the pool of possible libraries to use. 
\par Despite the wide variety of publicly available machine learning libraries, many do not have APIs for lower-level languages such as C/C++. While this is rapidly changing with the development of more embedded machine learning applications, at the time of the initial CE construction, this was the case \cite{tim_paper}. Furthermore, many of the existing C/C++ libraries lacked in-depth documentation for the lower-level APIs, had very complex build processes, or were built for larger, more complex applications (such as deep, convolutional NNs). 
\par MLPack \cite{cite_MLPAck_lol} was the library chosen to implement the MLPs in the CE. Because of the time of development the version used was \textbf{\textit{FIND OUT VERSION NUMBER LOL}}. Since then, MLPack has revamped its ANN architecture, so the library version is important when attempting to replicate these results. MLPack is built on top of the Armadillo \cite{cite_armadillo_lol} library, so Armadillo was used for self-written matrix/vector operations and storage as well. By doing this, the interfacing between self-written components and MLPack was simple. 
\par In order to simplify the interfacing that the CE has to do with Ethernet and UDP/IP for communication with modems (described in Section \ref{BG:hardware}), the Boost.ASIO \cite{cite_boostasio_lol} library was used. This library abstracts away any operating system-specific constructs for handling sockets.
\par Finally, the Boost.Serialization \cite{cite_boostSerial_lol} library was chosen for saving/resuming CE states. 
\begin{figure}
\caption{baseline implementation}
\end{figure}
\par A block diagram of the baseline implementation is shown in figure \ref{figure2FromTimsPaper}. The external drivers interact with the Viasat DVB-S2 modem (for getting $E_s/N_o$ values), the ML-605 BPSK transmitter (for sending new actions to the space DVB-S2 transmitter), and the Advanced Radio for Cognitive Communications (ARCC) platform created by NASA for initial ground testing. The cognitive algorithm under test resides in the RLNN Core section. The description of the RLNN Core will be split into three parts: the baseline method used in 2017, the RLM changes, and the Learn++.NSE changes.
\par The explore and exploit MLP ensembles were grouped into an NN Predictor object, allowing for the MLPack libraries to be abstracted out. Within the NN Predictor object is the FeedForwardNetwork object, which implements the individual FFNN (MLP) and the self-written implementation of Levenberg-Marquardt. As described in \ref{bg:onlineLearning}, the CE uses online training. This required two NN Predictor model, one to get trained up and one for execution of the model. This allows for concurrent training and usage. Training occurs using one NN Predictor model, and then the weights get copied to the other NN Predictor model. 
\par The buffer of training data, which is common between Explore and Exploit NN Predictors, is implemented in a separate object. It holds the latest N unique actions. When training occurs, the buffer is split into training and validation sets for each type of MLP (as Explore and Exploit MLPs have different inputs and outputs). The Armadillo library is used to perform this splitting.
\par The Application Specific Module provides context for the application, in that the rest of the RLNN Core is generic and can be applied to problems outside of communications. The Appication Specific module is the component that transforms communication metrics into the fitness scores that the RLNN Core uses. Any patches that change system behavior occur in this module as well. One example of this is the patch that limits transmit power changes to 1.5 dB steps. Another example is that the fitness scores are zeroed out when BER was measured to be 0.5, as this implies that the system had no communication between transmitter and receiver. 

\par The key changes between the RLM implemenation and the normal LM implementation occurred in the FeedForwardNetwork object. Here, the LM function was replaced by a function that runs RLM instead. Because of the additional parameters that RLM uses, a separate class called RecursiveLMHelper was used to contain the parameters and abstract the matrix math in the RLM updating process.

\par Unlike RLM, Learn++.NSE was mostly implemented in a modified version of NeuralNetwork Predictor, renamed LearnNSEPredictor. This is because the foundational MLP aspects of training are the same as the LM implementation. Where the difference occurs is the weighting of the samples that are being trained on and how the predictor is used. As such, the main changes occur in the train and predict functions. The algorithm is described in \ref{bg:NSESection}, and so will not be reiterated here.
\section{Hardware Methods}\label{methods:hardware}
\par In the following section, the hardware used in the ground testing and the on-orbit testing.
\subsection{ground test setup}

\par During the ground tests that were conducted during July of 2018, the test setup was very similar to that used in 2017 \cite{tim_implementation}.  Similarly to that work, a testbed at NASA GRC was used to create an emulation of the expected on-orbit environment. A simplified block diagram of the setup is shown in Fig. \ref{methods:groundTestFig}. The CE is contained in an Ubuntu 16.04 virtual machine on a Windows 7-based Dell T3600 Precision workstation. The VM was allocated 4 GB of dedicated memory, and shared all eight of the CPU cores (with hyper threading) with the host operating system. The CE is put on a private local area network with the ML-605 BPSK transmitter and the ViaSat DVB-S2 receiver. The ML-605 radio transmitted new actions to the S-band Radio engineering model \textbf{\textit{MAKE SURE THIS IS CORRECT TO WHAT WE ACTUALLY DID}}. The S-band Radio then transmitted DVB-S2 frames through a channel emulated by a variable attenuator, according to a measured SNR profile from a previous on-orbit experiment. The noise floor can be adjusted by the noise generator. All RF signals in this test were passed using coaxial cables, insetad of any antennas.
\begin{figure}[ht]
\caption{placeholder.}\label{methods:groundTestFig}
\end{figure} 

\subsection{Flight Setup}
\par The setup for on-orbit testing was identical to the setup used in 2017 \cite{tim_implementation}, which in turn used a setup initially used by previous NASA GRC collaborators. A simplified block diagram of the setup is shown in Fig. \ref{methods:flightTestFig}. In the chain, there are two DVB-S2 rx modems. The ViaSat modem was used for sending $E_s/N_0$ measurements at a rate of 100 Hz over UDP. The Newtec modem demodulates and decodes the actual frames coming in, and saves it to a binary file for postprocessing. During each dataframe, the CE saves the previous action tuple along with its performance. It then chooses the next action tuple, and sends the next action to the ML-605 BPSK modem, which then is used to uplink the signal to the SCaN Testbed.

\begin{figure}[ht]
\caption{placeholder.}\label{methods:flightTestFig}
\end{figure} 

\section{Post-processing techniques??? I'm not sure if this belongs in results}
