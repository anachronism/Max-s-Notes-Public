# Model priors and expectation maximization
* Have some data D
* Want best estimate of P(X), Theta.

* 2 estimates, Theta1 and Theta2.
* Compare P(Theta1) and P(Theta2) 
* which is more likely. 
* Can't tell which is better. We have data in addition to the estimate.
* What is  theta that maximizes over theta (argmax theta) of P(Theta|D)
* Using Bayesian rule.
        * P(A|B) = P(B|A)*P(A)/P(B)
        * P(Theta|D) = P(D|Theta) * P(Theta)/P(D)
        * How likely is the data given the Theta * How likely the parameter value / how likely data is.
* In a CS mindset, ignore P(theta) at first, by assuming all values equally likely (MLE)
* P(D) is constant over Theta, so ignore it.
* Must be careful about normalization now that terms are getting dropped. P(Theta) may go back in (MAP).

* Because of this, can look at the challenge as argmax Theta P(D|Theta)
* This is useful for evaluating hypotheses, but the probability returned is not the same as the probability of the model being correct (because we dropped out things).
* In ML, always assuming the data are independent of each other (so we can multiply probabilities).
* Any 0 probabilities sets things to 0, because frequently only sampling and getting a 0 just means we didn't sample enough. Often, zeros are replaced with real small values.
### Full bayesian
P(next is cherry) = P(cherry|h1)*P(h1) + P(cherry|h2)*P(h2)
* Sum of (likelihood of dist being h * likelihood of drawing cherry from h)
### Maximum LIkelihood
P(next is cherry) = P(cherry|hmax)
* Theta represents the probability distributions getting drawn from.
* Log likelihood is a good way of representing probability of one distribution.
  * when you add samples (but keep the ratio the same), the curve looks the same, just with different scale. Makes it easier to avoid overflow.

### Trickier problem
* 3 options: wrapper color(r/g), hole in middle (y/n), flavor (c/l)

## Hidden Variables
* we have been able to observe aspects, but what about what aspects we can't observe.
* Imagine there's latent variables. (They can greatly reduce number of parameters). 
* Sometimes latent variables are unobservable. Can make our lives easier, and are often how things really work. (Ie heart disease influencing symptoms, influenced by actions.
* But harder to model. 

* New candy problem. Candies are drawn from 1 of 2 bags. Each draw one bag is randomly selected (some probability). Then select random candy.
  * p(bag1) = X
### EM:
* Why: Used with latent variable cases.
* Start with initial guess of probabilities you're trying to find out.
* With these probabilities, compute how each item would be assigned to each latent class given current guess.
* Revise probabilities to maximize odds that the data was observed.

* In example, make up probability that it was drawn in bag 1/bag 2, probability cherry from bag 1, probability red in bag 1, probability cherry from bag 2, etc
#### THIS IS EXPECTATION:
* With what's observed, assign candies to coming from bag 1 and bag 2. Eg, figure out how many candies of each type come from bag 1 vs bag 2. 
  * Read: Get probability of red cherry hole candy coming from bag 1, bag 2, then normalize the two probabilities to sum to 1, and then multiply number of candies that fit that criteria with the probability of bag 1 to get the number of candies that are that criteria that are from bag 1.
* Expectation tends to be floats
* Given probabilistic model ,how likely was it to get the result we got? (Get log likelihood)
* For each cell, you get sum of p(|bag1),p(|bag2), then get log of it.
#### NORMALIZATION:
* Now pretend like we have actually observed everything from the expectation table.
* Re-estimate the probability parameters: p(bag 1) = sum(candies in bag 1)/total candies  
* Repeat this for all probabilities in table.

#### Details:
* Repeat. With new probability, new expectation is decided. Then, new maximization.
* Set threshold, if Log LIkelihood doesn't improve by some amount, it stops.
* With EM you often run it multiple times, to verify operation.
* Sometimes the algorithm has a hard time of determining which is bag 1 vs bag 2.
* Starting conditions affect where the model converges to. 
  * Log likelihood can converge, but differences between runs is large. 
* The problem is underdetermined, in that the solution could be just bag 1 is correct and bag 2 doesn't exist.
* Latent variables mean that the problem is underdetermined. EM can only find local maxima.

#### More.
* With a prior, bias the model towards certain thetas over others (weights things towards more reasonable answer).
* Enough data will reduce effect of prior. 
* Parameter estimates are not monotonic (but log likelihood is).
### Overview:
* Create initial model of theta (theta_0)
* arbitrarily, randomly, etc.
Use model theta to obtain another model theta such that log likelihood is an improvement from the last model.
* Repeat above step.
* Often used with clustering (soft clustering).
  * Run with different hypotheses, 1 cluster -> 2 clusters, etc. Wait until they improvement is small.
#### clustering:
* Guess mean and variance for each of the clusters.
* see how likely each data point is under this model.
* recompile variance and mean
#### Properties:
* EM is monotonic (for model fit).
* Each step moves towards the goal overall, but maybe not for each parameter.
* does not find global maxima.
* Statistical learning is interesting approach. We're at the intersection of machine learning and AI.
* Connects several areas (Bayesian networks, learning, stats, info theory(?))
### MLE problems:
* Sparse data does not get handled well.
* or 0 probabilities
* ignores info about likelihood of hypotheses.
* would like to incorporate priors