# IDK what this section is going to be yet.
* Random forests don't need a lot of tuning.
* tends to work out of the box.
* Try simple before more complicated.
* Cross validate.
* want function that generalizes well (eg pick the simplest hypothesis that fits the data)
* On training data, accuracy tends to increase with complexity.
  * but not true with validation /testing set hurr durr
* BIC is an analytical method to balance accuracy and complexity.
## Emperical examples of model goodness.
* Unseen test set is helpful.
  * But most machine learning techniques tend to work better w/ more data.
* How about best of both worlds?
## CV
* 10 fold cross validation:
  * Train on 9 of them, test on last one.
  * Trouble when randomly assigning folds with time series data.
  * Non stationarity and other things.
  * There's an assumption that folds are independent.
  * If not, maybe put all data from one type/user all in one fold.
* Then iterate, change which fold it gets tested on.
* No reason for 10 vs 100, 3, etc
* Larger number of folds runs slower but there's more data to train with.
* Accuracy is higher on training set than on CV set (duh)
* Unpruned makes better for training, worse for test.
### Limitations.
* Test data needs to really be unseen.
* Testing a lot of techniques with cv, and reporting accuracy of the one that does best, still can be overfitting. 
* Read up on train-tune-test methodology.
# Bootstrapping
* Useful without lots of statistics.
## Key idea:
* how likely is certain outcome?
* what is range of possible outcomes?
* Just need some data.
