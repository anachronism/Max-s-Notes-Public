# Supervised Learning, Q learning
* hard for model to deal with biased data (and hard to boil down to one number)
## Quick look at decision trees
* C4.5:
* Can represent any expression of input attributes
* there is a consistent tree for every training set with one path to each leaf (unless tree is nondeterministic)
* tree did worse with biased data than naive bayes.
* Takes set of cases that are availiable, test on different variables (each branch is a test). 
* Which attributes are good? Typically ones that are more separate (ie not the same results with different splits). Pure groupings.
## Inductive Learning
* Given set of observations, come up with model h that describes them.
* What does "describes" mean? 
* h is the same function as the one that generates them.
  * Bad becauseIn general, do not know which function would be better.
* change h to give good model for both current data, and likely good in the future.
* determining what to generalize is hard.
## Model Fit checking
* R^2
* percent correct
  * kappa (better percent correct)
* BIC, AIC
* AUC, A'
* F score
## Benefits:
* Lin reg easy to understand
* Robust
* good baseline
* Usually multivar.
## Simple Baselines:
* Naive Bayes
* C4.5
* Logistic reg
* lin reg
 * Can introduce higher order things (x^2, etc)
 * If you introduce too many higher order things, the models can perhaps start overfitting.
# Back to RL:
* Active learning: needs model for evaluation.
* learn function that predicts utility given state and action.
* instead of the state it ends up, use state it comes from.
## Sarsa:
* state action reward state action.
Q(s,a) = Q(s,a) + alpha * (R(s) +  gamma * Q(s',a') - Q(s,a))
* Q(s,a) is current value estimate.
* Q(s',a') is value of next move.
* don't use max because we don't care about what it might do.
* don't necessarily know what actions do, just utility values.

