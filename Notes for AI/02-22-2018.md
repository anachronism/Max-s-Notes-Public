# Probabilistic Stuff
* Exact inference in Hayes nets is NP-hard.
* NP hard is likely in exponential time.
* Implications: for large networks, exact inference not practical.
-> Approximation becomes needed.


## Gibbs Sampling
* Knowing if grass is wet, which nodes are relevant. 
        * Season, Rained, sprinkler on, slippery grass, prank, broken windows (influences pranks, which influences chance of grass being slippery but not wet. (Ie everything in the whole network, as whole thing is connected).
* What is the minimal set you need to know.
        * Rained, sprinkler on, slippery grass, Prank.
        * Season, and wet grass are conditionally independent. 
        * Prank is important because we assume wet grass slippery and prank based slippery are disjoint, so knowing if pranks are happening will change probability of wet grass.
        * With multiple causes, if one cause is true, the other causes are less likely.
        * Called "Explaining Away"
* Minimal set is called Markov Blanket.
        * Consists of Parents, Children, and Children's Parents.
* To infer about a node, no longer need to know about the entire network.
        * Still an NP-Hard problem, but smaller.
* Markov Blankets don't quite do what we want. 
* Don't know vals of all other nodes in blanket.
* Now, combine easy local comp with simulation.
## Gibbs sampling Algorithm
1) Set evidence nodes and never change. (Eg Prank in the class example)
2) Randomly assign other nodes (can ignore cond prob table)
3) For random node,
        * Randomly assign value conditioned on its Markov blanket (which are kept constant during this step, but are in list of nodes that can be assigned).
        * Update costs of values this node has taken.
        * Loop to 3 for N times.
* Compute how many times each node has taken on that value. 

### Example:
P(Wet = true | cloudy = true)
* Hold cloudy to be true.
* then randomly select other nodes).
* Start at sprinkler, 
        * S is sprinkler, C is cloudy, W is wet, R is rain.
        P(s) = \alpha P(S|C)*P(W|R,S)
        * P(S=t) = alpha P(S = t| C = t)* P(W = f | R = t, S = t)
        * P(S = f) = alpha P(S=f|C=t)*P(W=f|R=t,S=F)
        * The non-s values were assigned randomly, held constant.
        * So all these elts can be looked up in the table.
        * Calculate P(S), then randomly sample uniform dist and apply truth value that's chosen.
        * Then save the value it produced.
* Move on to next variable, repeat last steps.
* Look at wet, Markov blanket changes. 

### Convergence:
* Initial estimates are random.
* after large N, will start to converge.
  * Discard first n much less than N iterations (because they're not relevant).
  * Take average of assignments of the variable in question.

## Adding time to Bayesian Nets
* Assumes the past is not important, based on what we've looked at so far.
* Many processes are temporal, how to model?
* Can stick with causal links, eg multiple links given.
* Assume conditional independence often.
*  Figure (1) is first-order Markov process. ( in notebook).
* Create evidence nodes to observe the actual nodes we care about (Hidden Markov Model).
    * EG determining what weather is based on what you see out the window, (2)
* Each time step is a slice.
### Sensor model: P(observed\_var | latent\_var)
* Don't invert arrows from observable to latent (since observation is dependent on latent cars)
* Also, with latent pointing to observable, adding an observable will just mean making a new sensor model.
* Assuming sensor model is the same across time slices.
### Transition Model: P(x\_{t+1}|x\_{t})
* Probability of the current step given the last step.
* We assume transition model is constant (which doesn't assume the world is constant).
### Inference:
* For non-temporal, not a lot of types.
  * Exact inference, rejection sampling, Gibbs sampling
  * Same Q, different methods
* Temporal have many different Qs.
### Filtering:
* Given all observations up to time t, what is best est of state at time t.
* Markov blanket of P(X\_{t+1}) is X\_{t}, and e{\_{t+1}.
* P(X\_{t+1}|e\_{1:t+1}) = alpha P(e\_{t+1}|X)*Sigma(P(x\_{t+1}) * P(x\_t|e\_{1:t})
* Initial start state impacts beginning, but it self-corrects. Does affect how fast it converges though
* sensor data can sometimes drop out and the model still operates.
### Kalman Filter:
* Typically, location est.
* have prior est of location, how it moves (transition model).
* Have sensor model (sensed location).  
* The actual location is somewhere in between these two models.
    * New pos = B*sensor model + (1-B) * transition model
    * So, how to select B?
* Sensor model: often phrased in terms of Gaussian error. Assuming unbiased model. 
* Transition model F(x\_t)
* B = (var(t) +var(x))/(var(t) + var(x) + var(z)) 
    * Var(t) is err in pos, Var(x) is error in transition model, Var(z) is err in sensor reading.
  * Numerator can be considered noise in transition models est, denominator is total noise.
* Vocab: B in this is Kalman gain. High K means sensors are good.
* Position can be represented as a Gaussian (but with odd noise, weird solutions).
* Sometimes The Gaussian assumption is incorrect (chi^2, poisson, etc)
* KF are applicable as filtering for pretty much any contiuous values.
  * Anything you're trying to do state estimation on.

### Future values:
 * Prediction of future values.
    * Pretty much, further you go, harder to predict, converges to a value.
    * Only have transition model, converging to stationary point. S = p2/(p1-p2-1) 
    P1 = p(e|x), p2 = p(e| not x)
### Problems with filtering/monitoring.
* Not robust to outliers.
* Becomes smoothing  issue.
    * Know past points, future points.
### Smoothing
* Smoothing maximizes likelihood taking into account future data, but filtering does not take into account future data.
* Smoothing is where was I, Monitoring was where am I, prediction is where will I be.
* More vocab: Dynammic Bayesian Networks (DBN): Generalization of HMMs. Relax several restrictions.
    * can use an arbitrary DAG for each slice.
### Recap:
* Bayesian networks is rich modeling framework.
* Broad tool.
* Talked about how to introduce time, extending things.
* Be thoughtful based on the problem you have to deal with.
