# Practicality
* NxM gridworld takes O(NM) time per iteration.
* Arprox O(N+M) iterations to propagate signal to goal.
* Roughly O(N^3)
* Roughly takes O(# states) to process since must visit each state.
* This is mainly only good for small problems.
## Goal:
 How to use RL in bigger things.
## Passive reinforcement learning:
* learner has no control over the actions performed.
* Learn utility of states under whatever policy is being followed. (Doesn't control).
## Driving is next example. 
* easy to think more broadly about RL
* Reward fun = 100 - minutes traveled.
## Direct Utility estimation (Monte Carlo/Widrow hoff).
* examine the end of the trajectory, then sum rewards received. Use that as estimate.
* Robust, works.
* get utility estimate for each spot. 
* Reward is rule of environment (vs utility being more like a heuristic).
* Efficient vs Value iteration because value iteration visits every node equally, while DUE only looks at paths that were visited.
* Several trajectories are needed to actually make it work.
* Also, DUE doesn't reuse information. (this is all passive).
Instead, use something that is similar to bellman's eq.
## Temporal Difference Learning
U_new(s) = U(s) + alpha * (R(s)+gamma*U(s_prime) - U(s))
* Alpha is step size,U(s_prime)-U(s) is difference, R(s) is reward, gamma is future discounting factor, alpha is step size.
