# Practicality
* NxM gridworld takes O(NM) time per iteration.
* Arprox O(N+M) iterations to propagate signal to goal.
* Roughly O(N^3)
* Roughly takes O(# states) to process since must visit each state.
* This is mainly only good for small problems.
## Goal:
 How to use RL in bigger things.
## Passive reinforcement learning:
* learner has no control over the actions performed.
* Learn utility of states under whatever policy is being followed. (Doesn't control).
## Driving is next example. 
* easy to think more broadly about RL
* Reward fun = 100 - minutes traveled.
## Direct Utility estimation (Monte Carlo/Widrow hoff).
* examine the end of the trajectory, then sum rewards received. Use that as estimate.
* Robust, works.
* get utility estimate for each spot. 
* Reward is rule of environment (vs utility being more like a heuristic).
* Efficient vs Value iteration because value iteration visits every node equally, while DUE only looks at paths that were visited.
* Several trajectories are needed to actually make it work.
* Also, DUE doesn't reuse information. (this is all passive).
Instead, use something that is similar to bellman's eq.
## Temporal Difference Learning
U_new(s) = U(s) + alpha * (R(s)+gamma*U(s_prime) - U(s))
* Alpha is step size,U(s_prime)-U(s) is difference, R(s) is reward, gamma is future discounting factor, alpha is step size.
Benefits:
* No need for model of environment.
* Computationally cheap
* works
* General RL goal. Given state S, what's best choice?
* no need for state to be accurate, just that the right choice is made.
* Val iteration focuses on all things that COULD happen (aka Dynamic programming)
* TD focuses on what DID happen.
# Active Learning
* Changes to what we've talked about:
 - Need to select action maximizing reward value.
 - Needs to explore some of the domain (to make model more representative).
 - Explore vs exploit
## Approach 1: epsilon Greedy
* Pick random action w/ small probability epsilon.
* Probably want to explore more at beginning, so can vary epsilon based on conditions.
## Approach 2: Optimistic Initial Estimates
* If state has been visited less than K times, assign it a large reward value to prioritize exploring it.
* Else, assign actual value.
* Can assign the reward value to prioritize decisions that make intuitive sence.
* Initial estimate doesn't need to be uniform.
## Differences:
* OIE initially explorse more at the beginning.
 * But after initial exploration, will write off bad paths, and eventually stops exploring.
### In a nonstationary environment:
* if best case gets worse, given a proper exploration, the agent can deal with it.
* If another case gets better, epsilon greed ywould eventually find it (OIE would not).
* With TD learning, need to explore a lot to get this new best value.
* Nonstationarity introduces an assymetry.
