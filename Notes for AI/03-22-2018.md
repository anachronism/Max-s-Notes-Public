# Practicality
* NxM gridworld takes O(NM) time per iteration.
* Arprox O(N+M) iterations to propagate signal to goal.
* Roughly O(N^3)
* Roughly takes O(# states) to process since must visit each state.
* This is mainly only good for small problems.
## Goal:
 How to use RL in bigger things.
## Passive reinforcement learning:
* learner has no control over the actions performed.
* Learn utility of states under whatever policy is being followed. (Doesn't control).
## Driving is next example. 
* easy to think more broadly about RL
* Reward fun = 100 - minutes traveled.
## Direct Utility estimation (Monte Carlo/Widrow hoff).
* examine the end of the trajectory, then sum rewards received. Use that as estimate.
* Robust, works.
* get utility estimate for each spot. 
* Reward is rule of environment (vs utility being more like a heuristic).
