# Intro to Probability (Let's see if I end up leaving first)
## Background
* Uncertainty: hard to take into account 
* P(Bucharest) = 0.95
* P(pitesti) = 0.04
* P(elsewhere) = 0.01
* But what does this probability imply? (Well, good way to think of it anyways).
* Logic is encapsulated in probability (P(0) is false, P(1) is true).
* Probability summarizes effects of laziness and ignorance.
        * failure to enumerate exceptions, etc
        * lack of relevant facts, in it conditions, etc.
* Where do numbers come from?
  * Frequentist (experimental)
  * Subjectivist (consult experts)
* Prob axioms.
  *  p(A) from [0,1]
  * P(true) = 1, P(false) = 0
  * P(AuB) = P(A)+P(B) - P(A^B)
## Considering multiple variables.
* Joint probabilities:
* given toothache, catch or not catch.
* LOOK IN NOTES IN BOOK
* p(no cavity | toothache) and p(cavity|toothache) both use p(toothache).
  * Can avoid dividing, and then add in a normalization factor.
## What to do with probabilities?
* Multiple actions with different probabilities, how to pick which action is "better"
* Best choice depends on preferences.
* figured out how to use utilities to make decisions.
* Still, how to make them more practical?
  * Because number of entries is exponential (when compared to number of parameters)
  * Major problem. 
    * Solution: independence assumptions.
### Independence
* A and B independent iff P(A|B) = P(A), or P(A,B) = P(A)P(B) 
* Implying independence is that you can pull out, resulting in two tables.
  * Tables containing variables that are dependent on each other.
* Assuming independence tends to bring exponential down to linear.
* But many things are not independent.
## Correlation does not imply causation lmao
* Shoe size and spelling ability are conditionally independent given age (not independent), 
  * This means P(good speller | shoe, age) = P(good speller | age)
  * Given age, good speller and shoe size are independent
* Conditional independence reduces number of parameters.
  * toothache example in written notes.
* goes from 31 joint parameters to:
  * 3 for weather, 1 for cavity, 2 for toothache, 2 for catching
  * Nodes with a lot of arrows in are complicated to model.
  * 8 parameters.
* Bigger example
  * alarm is ringing, called by john, Mary doesn't call
  * sometimes go off with earthquakes. Is there a burgarlar.
## Bayesian nets
* Enable us to answer arbitrary queries about the data.
* Can do exact, but NP-hard
* Let's approximate.
  * One way is to use sample mean.
  * More trials reduces variance.
  * Error typically proportional to 1/sqrt(N)
* Diminishing returns.
* Rare events hard to estimate.
  * Either 0 or a number larger than actuality.
* Estimating probability, in notes, slippery?

## Rejection Sampling Algorithm
Valid samples = 0
For l = 1:n samps
  * randomly instantiate root
  * go through rest of net, randomly instantiate.
  * throw out irrelevant samples (non consistent assignment).
## Problems
* throw out a lot of samples with inconsistent samples, samples remaining may be niche.
* Can't Set deterministic vals to nodes (instead it's randomly sampling but only looking at cases where X is true).
* More effective techniques.
  * MCMC (Gibbs sampling)
  