# Game Playing and multi-agent search
* Search isnt' always actually searching for something.
* Instead, considering what's next.
## Classic (week 3)
* Consider various moves, consider implications, select action that will do best thing.
## Game Playing
* In games, people working at cross purposes.
* Essentially, just two different utility functions.
* Not just games.
* Huge search space (branching factors 15-80 common, may have to search many moves ahead).
* Not searching whole tree.
* How to efficiently allocate time.
        * What helps the most in the last amount of time?
* Assuming opponent will play optimally. 
        * People don't always play optimal IRL
        * Also assuming that the numbers aren't all correct.

* test edit for git committing *  test test test

### MINIMAX
* Some leaf nodes matter, others do not (eg if number changed isn't the decisive node).
* Complete, O(bm) space complexity, Optimal, Time complexity O(b^m)
* Why take the min?
        * Rational opponent lol.
* Assuming rational opponent is not necessarily good irl.
### AlphaBeta
* First option is at least 3.
* Second is less than/equal to 2, just looking at the first number that is 2.
* Pruning doesn't affect final result.
       * If only searched same amount.
* With Perfect ordering, time complexity = O(b^{m/2}. Doubles depth that's capable of search.
* Equivalent to a branching factor of root(b). 
### Fundamental truths
* Trees are not exponential.
* Can prune, restrict depth, restrict breadth, etc.
* Terminal states are easy to evaluate.
        * Harder to evaluate intermediate game state.
* EG chess, points on pieces are not exactly tied to winning or losing. Evaluation functions are kind of estimates.
* How to evaluate how good a position is. Humans consider few moves, but can play games well.
        * Eval fcn is key.
        * A lot of possibilities.
* eval fcns tend to be easy (because exponential executions).
* before we assumed leaf nodes were terminal states, but what if they're evaluation function values instead?
        * Since the values aren't directly correlated (but are actually estimates), then "optimal" decisions no longer correct.
        * Start introducing uncertainty values (but those add time).
* In real life programs, minimax/alphabeta are no longer optimal. 
* Bottom of search may result in crazy moves that look good but are actually bad (eg taking bishop with queen but putting the queen in a poor position). 
## Quiescence search.
* Some positions are dynamic (search deeply and narrowly).. Spend more time here.
        * If last move is a capture, keep searching on sequence of captures.
        * Eg go deeper when there's a crazy move.
* determines when you have to search more deeply. 
* Problem. How to restrict breadth?
        * WE can restrict depth/ how to focus search.
* Backgammon is combinitorial search (comparable to go). Most parts of the space are meaningless.
* Instead of looking at all possible outcomes, just look at one for each start direction.
        * This is the simulation. 
        * At each move that's not random chance, a heuristic is chosen and used to pick specific moves. This allows search to the end.
        * repeat sim N times, go with most promising option.
* This is called rollout. (this one has some heuristic). 
* Monte Carlo is search with no heuristic (but requires smart way to do it). 
        * Looks at actions that look better, but also looks at underexplored actions eventually, a la explore-exploit.
#### Pros/cons of rollouts vs iterative deepening.
* Pros: time complexity is reduced, can give probabilistic estimate. Can use utility state, don't need heuristic (sort of), introduces noise (easier to scale too), considers long-term impact of actions.
* Cons: Not optimal (dependent on utility), misses a lot of possible outcomes (even fairly shallow outcomes)[doesn't make use of feedback], sensitive to heuristic function (since only one choice made).
* Protocol:
1) Roll dice once.
2) select good move based on heuristic.
3) repeat lol
* Useful when there's a component that is effectively random.
* Can be broadened if a lot of possible things can happen.
* No need for purist, can combine brute force search + rollouts, etc, with alpha-beta, etc.
## Recap
* restrict search depth (need eval fcn)
* Can restrict breadth, rollouts/monte Carlo, beam search (elitism for search).
* Focus on fruitful areas: alphaBeta pruning, quiescence search.
### Why not apply heuristic function at each step?
* Usually have extra time to search down a little.
* Heuristics miss things.

## Logic
* crow(x) -> black(x).
* Encodes a lot of statements as sentences.
* very flexible.
* Tend to be very strict and wrong.
* A^B^C -> D (if a&b&c then d)
        * Not great in practice.
* Eg drive(potential,Bucharest) -> location(Bucharest)
        * not always correct.
        * Even if you correct, hard to confirm the list conditions (list is not exhaustive).
* Pretend all other things work 
        * But what if no.
* Hard to list all exceptions (laziness), and can't know (ignorance, theoretic and practical).
* Next up, probabilistic inference (probability stuff).
=======
# Game Playing and multi-agent search
* Search isnt' always actually searching for something.
* Instead, considering what's next.
## Classic (week 3)
* Consider various moves, consider implications, select action that will do best thing.
## Game Playing
* In games, people working at cross purposes.
* Essentially, just two different utility functions.
* Not just games.
* Huge search space (branching factors 15-80 common, may have to search many moves ahead).
* Not searching whole tree.
* How to efficiently allocate time.
        * What helps the most in the last amount of time?
* Assuming opponent will play optimally. 
        * People don't always play optimal IRL
        * Also assuming that the numbers aren't all correct.

### MINIMAX
* Some leaf nodes matter, others do not (eg if number changed isn't the decisive node).
* Complete, O(bm) space complexity, Optimal, Time complexity O(b^m)
* Why take the min?
        * Rational opponent lol.
* Assuming rational opponent is not necessarily good irl.
### AlphaBeta
* First option is at least 3.
* Second is less than/equal to 2, just looking at the first number that is 2.
* Pruning doesn't affect final result.
       * If only searched same amount.
* With Perfect ordering, time complexity = O(b^{m/2}. Doubles depth that's capable of search.
* Equivalent to a branching factor of root(b). 
### Fundamental truths
* Trees are exponential.
* Can prune, restrict depth, restrict breadth, etc.
* Terminal states are easy to evaluate.
        * Harder to evaluate intermediate game state.
* EG chess, points on pieces are not exactly tied to winning or losing. Evaluation functions are kind of estimates.
* How to evaluate how good a position is. Humans consider few moves, but can play games well.
        * Eval fcn is key.
        * A lot of possibilities.
* eval fcns tend to be easy (because exponential number ofexecutions).
* before we assumed leaf nodes were terminal states, but what if they're evaluation function values instead?
        * Since the values aren't directly correlated (but are actually estimates), then "optimal" decisions no longer correct.
        * Start introducing uncertainty values (but those add time).
* In real life programs, minimax/alphabeta are no longer optimal. 
* Bottom of search may result in crazy moves that look good but are actually bad (eg taking bishop with queen but putting the queen in a poor position). 
## Quiescence search.
* Some positions are dynamic (search deeply and narrowly).. Spend more time here.
        * If last move is a capture, keep searching on sequence of captures.
        * Eg go deeper when there's a crazy move.
* determines when you have to search more deeply. 
* Problem. How to restrict breadth?
        * WE can restrict depth/ how to focus search.
* Backgammon is combinitorial search (comparable to go). Most parts of the space are meaningless.
* Instead of looking at all possible outcomes, just look at one for each start direction.
        * This is the simulation. 
        * At each move that's not random chance, a heuristic is chosen and used to pick specific moves. This allows search to the end.
        * repeat sim N times, go with most promising option.
* This is called rollout. (this one has some heuristic). 
* Monte Carlo is search with no heuristic (but requires smart way to do it). 
        * Looks at actions that look better, but also looks at underexplored actions eventually, a la explore-exploit.
#### Pros/cons of rollouts vs iterative deepening.
* Pros: time complexity is reduced, can give probabilistic estimate. Can use utility state, don't need heuristic (sort of), introduces noise (easier to scale too), considers long-term impact of actions.
* Cons: Not optimal (dependent on utility), misses a lot of possible outcomes (even fairly shallow outcomes)[doesn't make use of feedback], sensitive to heuristic function (since only one choice made).
* Protocol:
1) Roll dice once.
2) select good move based on heuristic.
3) repeat lol
* Useful when there's a component that is effectively random.
* Can be broadened if a lot of possible things can happen.
* No need for purist, can combine brute force search + rollouts, etc, with alpha-beta, etc.
## Recap
* restrict search depth (need eval fcn)
* Can restrict breadth, rollouts/monte Carlo, beam search (elitism for search).
* Focus on fruitful areas: alphaBeta pruning, quiescence search.
### Why not apply heuristic function at each step?
* Usually have extra time to search down a little.
* Heuristics miss things.

## Logic
* crow(x) -> black(x).
* Encodes a lot of statements as sentences.
* very flexible.
* Tend to be very strict and wrong.
* A^B^C -> D (if a&b&c then d)
        * Not great in practice.
* Eg drive(potential,Bucharest) -> location(Bucharest)
        * not always correct.
        * Even if you correct, hard to confirm the list conditions (list is not exhaustive).
* Pretend all other things work 
        * But what if no.
* Hard to list all exceptions (laziness), and can't know (ignorance, theoretic and practical).
* Next up, probabilistic inference (probability stuff).

