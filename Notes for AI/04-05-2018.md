# Cross Validation
* Random forest needs few tweaks.
* generally works ok.
* Want to fit the training data (confusion matri and % correct, fscore, etc)
* don't want overly complex model.
* want a function that will generalize well. 
* accuracy increases with complexity on training set (but not on test data). 
* BIC also is also used to stop training.
* w/ test sets, cross validation can be useful. 
  * split data up to N folds (randomly)
  * Be careful with time series data when doing this.
  * leave one for testing, then train on rest.
  * Go through process, changing which fold is test.
  * Small N means that it will happen quickly, less data.
  * large N means more data used to train, but takes longer.
  * Increasing number of folds only increases performance so much ( asymptotic), because the amount of data used in training asymptotes.
* pruning is used to make trees more generalized. 
* CV really needs properly independent data. 
* testing a lot of techniques with cross validation.
  * For each fold going through, each cv will make the "unseen" fold more seen.
  * Read up on train-tune-test methodology instead.
# Bootstrapping
* essentially, draw new samples, get mean.
* Sample with replacement.
  * Repeat, then take sample mean of parameter you care about.
* Large Outliers effects on mean results:
  * the difference between two very large numbers relative to the rest of the dataset mean that they have less impact between, say, 2^100 vs 2^20
## Bootstrap algorithm:
* sample S of size S
For i = 1:N
  draw |s| samples from s w/ replacement.
  Compute something with temp.
  record result
## Bootstrap w ml
* sample S of size S
For i = 1:N
  draw |s| samples from s w/ replacement.
  train model with this drawn sample.
  record result
  
 * we now have bagging.
 * N slightly different models (trained on different data).
 * Hopefully these N models will make different mistakes.
  * Average should work.
 * Bootstrap is similar to rollouts/dealouts. Essentially sampling to see the range of things that may happen.
 * Adding more bags have diminishing returns (duh).
 ## Random vorest
 * take subset of data.
 * take subset of features
 * train decision tree.
  * THis tree probably wouldn't work as well as ones trained on all features.
  * each tree is weaker, power in numbers.
 * Training:
   for l = 1:M
    draw sample of data and features.
    train ith tree
   while predicting
    test item on all 100 trees.
    let them vote on most likely.
  * Similar to bagging, but also subset of features.
  * random forest overfits easier than bagging.
  * Training set results not very useful in this case.
  ## Common baselines
  * Naive Bayes, logistic regression.
  * standard lin regression, logistic transformation to convert to probability. e^x/(1+e^x)
  * logistic regression beat bagged, random forest, etc. 
  ## Take out.
 * trees can be used for regression (leaves a numeric prediction).
 * Model trees. Leaf nodes are a model that makes numeric prediction.
 * minimizing square may not be best option.
 * With RL, how to deal with more states than can be visited.
  * change concept of state (like every x,y pair separately).
  * Instead, learn y = B1 * x  + B2 * y (lin model).
  * Update lin model for each x,y pair update.
  * Pretty much, choose the correct features that make the function relatively easy to learn. 
  ## Conclusions:
  * ML applies broadly.
  * ML is tough problem, underfit and also overfit are bad.
