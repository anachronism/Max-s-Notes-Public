# Machine learning lol
* Unsupervised: what to learn just given some observations. (clustering, dimensionality reduction, etc)
* Supervised: observations with labels, try to be able to deal with labels.
## Supervised: naive bayes.
* Some outcome you want to predict. Outcome points to each of the features you're using to classify.
* once again, a lot of inputs is bad, so it points the other way.  
* Why is it Naive?
  * Assumes features are conditionally independent, so no other links in the model.
  * Not really a believable assumption.
* Naive bayes is essentially using the averages to make probabilities and then get probability.
P(play yes | evidence) = P(E|play is yes)*P(play=yes)/P(E)
* How to simplify? Assume cond independence and so each is separate.
* base rate(p(play=yes)) is important.
* have to figure out what probability(evidence) is to conclude anything.
* normalize (p(play=yes|evidence) + p(play=no|evidence = 1)
* This was bayes net and parameter fitting.
* could use more complex network structure and estimate parameters.
* Any bayesian net can be done with this.
* low count messes up naive bayes classifier.
  * replace zero counts with small numbers.

* When a lot more of one label occurs than the other, classifiers may become biased. (really good with a).
* Good baseline technique to compare how you're doing.

# RL:
* successs stories: Checkers, TD-gammon, chess, go.
* Helicopter control.
* Don't tell agents how to behave, but instead tell them what we want done.
* Vast difference in difficulty.
  * Diving vs judging.
  * determining the best move vs. determining who won.
  * Much easier to judge than to perform. (generator vs classifier).

## Grid worlds:
* easy in that start somewhere, good places, bad places, obstacles.
* Must prioritize the efficiency.
  * Can do by adding small penalty with motion.
* Reward signal.
  * pos values with good, - when something bad.
  * do whenever task "completes" eg completing points in ping pong, eg completing a game of chess.
    * Ping pong, if each point is won, it will likely win whole game.
    * In chess, capture doesn't necessarily improve game state.
* Goal is what the agent should do but not how it should do it.
* But how to add knowledge? Let agent figure out how it relates. Give agent the possible useful features, and let it figure it out.

Grid worlds are very variable.
  * observability (do I know where I am?).
  * Whether world is stationary.
  * Complexity of environment.
  * Determinism.
#EG Transition Model (determinism):
* 80% chance direction is right.
* 10% 90 deg left, 10% 90 deg right.
* Solvable even if you don't know where you are.
* Expected future reward isn't +1. (moves have penalties and are probabilistic).
  * Can't just sum it up, some systems are infinite length trials. Future is uncertain.
  * One option: discounting future rewards. Multiply rewards farther off by gamma ~= 0.9 etc. (kinda like momentum updating).
## lol
* what if we knew how good each state was.
* weight future how much you move forward.
* Way to judge if algorithm is done, it would be evaluating if the policy has converged.
* You wait until the policy to converge ( even if the value doesn't) so that you will work well under all conditions.
* 
